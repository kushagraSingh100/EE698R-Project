{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f62a34b1-a6ed-4fcc-a268-4b665a4773f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Applications/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /Applications/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Applications/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Applications/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Applications/anaconda3/envs/torch/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Applications/anaconda3/envs/torch/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Applications/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#To get features for single file\n",
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "\n",
    "def process_audio(audio_path, target_duration=1.5, sr=16000):\n",
    "    try:\n",
    "        waveform, _ = librosa.load(audio_path, sr=sr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {audio_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    target_samples = int(target_duration * sr)\n",
    "    \n",
    "    if len(waveform) > target_samples:\n",
    "        waveform = waveform[:target_samples]\n",
    "    else:\n",
    "        padding = target_samples - len(waveform)\n",
    "        waveform = np.pad(waveform, (0, padding), mode='reflect')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = processor(waveform, return_tensors=\"pt\", sampling_rate=sr)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().to(torch.float32).numpy()\n",
    "    \n",
    "processor = Wav2Vec2Processor.from_pretrained(\"theainerd/Wav2Vec2-large-xlsr-hindi\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"theainerd/Wav2Vec2-large-xlsr-hindi\")\n",
    "model.eval()\n",
    "def extract_and_save_feature(wav_path, output_dir=\".\", output_name=None):\n",
    " \n",
    "    features = process_audio(wav_path)\n",
    "    if features is None:\n",
    "        raise ValueError(\"Feature extraction failed.\")\n",
    "    \n",
    "    # Generate a default filename if not provided\n",
    "    if output_name is None:\n",
    "        output_name = f\"{os.path.splitext(os.path.basename(wav_path))[0]}_{hash(wav_path)}.npy\"\n",
    "    \n",
    "    output_path = os.path.join(output_dir, output_name)\n",
    "    np.save(output_path, features)\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f538922-9c79-4608-ad80-3547fbb24efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature saved at: ft/होंगी_844780148623943267.npy\n"
     ]
    }
   ],
   "source": [
    "#To get features for single file\n",
    "\n",
    "wav_file_path = r\"voice/होंगी.mp3\"  \n",
    "save_dir = r\"ft\"        \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "saved_path = extract_and_save_feature(wav_file_path, output_dir=save_dir)\n",
    "print(f\"Feature saved at: {saved_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b901804-2009-4fbc-84f5-260606704b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#To get features for multiple file in folder\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"theainerd/Wav2Vec2-large-xlsr-hindi\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"theainerd/Wav2Vec2-large-xlsr-hindi\")\n",
    "model.eval()\n",
    "\n",
    "def process_audio(audio_path, target_duration=1.5, sr=16000):\n",
    "    try:\n",
    "        waveform, _ = librosa.load(audio_path, sr=sr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {audio_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    target_samples = int(target_duration * sr)\n",
    "    \n",
    "    if len(waveform) > target_samples:\n",
    "        waveform = waveform[:target_samples]\n",
    "    else:\n",
    "        padding = target_samples - len(waveform)\n",
    "        waveform = np.pad(waveform, (0, padding), mode='reflect')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = processor(waveform, return_tensors=\"pt\", sampling_rate=sr)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().to(torch.float32).numpy()\n",
    "\n",
    "def extract_and_save_feature(wav_path, output_dir):\n",
    "    features = process_audio(wav_path)\n",
    "    if features is None:\n",
    "        print(f\"Skipping file: {wav_path}\")\n",
    "        return\n",
    "\n",
    "    file_name = os.path.splitext(os.path.basename(wav_path))[0] + \".npy\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "    np.save(output_path, features)\n",
    "    print(f\"Feature saved at: {output_path}\")\n",
    "\n",
    "def process_folder(input_folder, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.lower().endswith((\".wav\", \".mp3\")):\n",
    "            full_path = os.path.join(input_folder, file)\n",
    "            extract_and_save_feature(full_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5c006ea-0376-4a54-ab3e-2a06c6e762ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature saved at: ft/सरकारी.npy\n",
      "Feature saved at: ft/सड़क.npy\n",
      "Feature saved at: ft/होंगी.npy\n",
      "Feature saved at: ft/गुवाहाटी.npy\n",
      "Feature saved at: ft/अभिनय.npy\n",
      "Feature saved at: ft/जीवन.npy\n",
      "Feature saved at: ft/उदाहरण.npy\n",
      "Feature saved at: ft/आधिकारिक.npy\n",
      "Feature saved at: ft/ओलिंपिक.npy\n",
      "Feature saved at: ft/होली.npy\n",
      "Feature saved at: ft/कश्मीर.npy\n",
      "Feature saved at: ft/स्वास्थ्य.npy\n",
      "Feature saved at: ft/कोलकाता.npy\n"
     ]
    }
   ],
   "source": [
    "#To get features for multiple file in folder\n",
    "input_audio_folder = r\"voice\"   \n",
    "output_feature_folder = r\"ft\"   \n",
    "process_folder(input_audio_folder, output_feature_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978efb8-dfac-4efb-8472-496744a7bbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
